{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ehrapy with Large Datasets\n",
    "\n",
    "Modern health datasets can become very large. When datasets are so large they cannot be loaded into a computer's memory at once, loading and processing the data in batches becomes necessary. This is also called doing the computations \"out-of-core\".\n",
    "\n",
    "[Dask](https://www.dask.org) is a popular out-of-core, distributed array processing library that ehrapy is beginning to support.  Here we show how `dask` support in `ehrapy` can reduce the memory consumption of a simple ehrapy processing workflow.\n",
    "\n",
    "ðŸ”ª **Beware sharp edges!** ðŸ”ª\n",
    "\n",
    "`dask` support in `ehrapy` is new and highly experimental!\n",
    "\n",
    "Many functions in `ehrapy` **do not** support `dask` and may exhibit unexpected behaviour if dask arrays are passed to them. Stick to what's outlined in this tutorial and you should be fine!\n",
    "\n",
    "Please report any issues you run into over on the issue tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usecase\n",
    "\n",
    "We can now profile the required time and memory consumption of two runs for processing this data:\n",
    "\n",
    "1. In memory (which is feasible with our demo dataset)\n",
    "2. Out-of-core\n",
    "\n",
    "We will compare these two on a synthetic dataset of 50'000 samples and 1'000 features, with 4 distinct groups underlying the data generation process.\n",
    "\n",
    "On this dataset, we\n",
    "\n",
    "1. Scale the data to zero mean and unit variance\n",
    "2. Compute a PCA\n",
    "3. Compute a neighborhood graph on PCA space\n",
    "4. Perform clustering in the neighborhood graph\n",
    "5. Project the data to the top two Principal Components space, and color the found clusters for visualization.\n",
    "\n",
    "\n",
    "### Profiled Code\n",
    "\n",
    "#### Memory\n",
    "\n",
    "For the in-memory setting, the following code is used to generate profiling results:\n",
    "```py\n",
    "import scalene\n",
    "\n",
    "scalene.scalene_profiler.stop()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs as make_blobs\n",
    "import ehrapy as ep\n",
    "import anndata as ad\n",
    "\n",
    "n_individuals = 50000\n",
    "n_features = 1000\n",
    "n_groups = 4\n",
    "chunks = 1000\n",
    "\n",
    "data_features, data_labels = make_blobs(\n",
    "    n_samples=n_individuals, n_features=n_features, centers=n_groups, random_state=42\n",
    ")\n",
    "\n",
    "var = pd.DataFrame({\"feature_type\": [\"numeric\"] * n_features})\n",
    "\n",
    "adata = ad.AnnData(X=data_features, obs={\"label\": data_labels}, var=var)\n",
    "\n",
    "scalene.scalene_profiler.start()\n",
    "\n",
    "ep.pp.scale_norm(adata)\n",
    "\n",
    "ep.pp.pca(adata)\n",
    "\n",
    "ep.pp.neighbors(adata)\n",
    "\n",
    "ep.tl.leiden(adata)\n",
    "\n",
    "ep.pl.pca(adata, color=\"leiden\", save=\"profiling_memory_pca.png\")\n",
    "\n",
    "scalene.scalene_profiler.stop()\n",
    "```\n",
    "\n",
    "#### Out-of-core\n",
    "\n",
    "For the out-of-core setting, the following code is used to generate profiling results:\n",
    "```py\n",
    "import scalene\n",
    "\n",
    "scalene.scalene_profiler.stop()\n",
    "\n",
    "import dask.array as da\n",
    "from sklearn.datasets import make_blobs as make_blobs\n",
    "import ehrapy as ep\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "\n",
    "n_individuals = 50000\n",
    "n_features = 1000\n",
    "n_groups = 4\n",
    "chunks = 1000\n",
    "\n",
    "data_features, data_labels = make_blobs(\n",
    "    n_samples=n_individuals, n_features=n_features, centers=n_groups, random_state=42\n",
    ")\n",
    "\n",
    "data_features = da.from_array(data_features, chunks=chunks)\n",
    "\n",
    "var = pd.DataFrame({\"feature_type\": [\"numeric\"] * n_features})\n",
    "\n",
    "adata = ad.AnnData(X=data_features, obs={\"label\": data_labels}, var=var)\n",
    "\n",
    "scalene.scalene_profiler.start()\n",
    "\n",
    "ep.pp.scale_norm(adata)\n",
    "\n",
    "ep.pp.pca(adata)\n",
    "\n",
    "adata.obsm[\"X_pca\"] = adata.obsm[\"X_pca\"].compute()\n",
    "\n",
    "ep.pp.neighbors(adata)\n",
    "\n",
    "ep.tl.leiden(adata)\n",
    "\n",
    "ep.pl.pca(adata, color=\"leiden\", save=\"profiling_out_of_core_pca.png\")\n",
    "\n",
    "scalene.scalene_profiler.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Try it Yourself\n",
    "\n",
    "Click here for instructions of how to run the profiling results yourself.\n",
    "\n",
    "<details>\n",
    "  <summary>\n",
    "Workflow: \n",
    "\n",
    "  >>> 1. Setup\n",
    "  >>> The results shown in this notebook rely on optional dependencies of `ehrapy`. Also, we will use [scalene](https://github.com/plasma-umass/scalene#using-scalene) for profiling. You can install these required tools into your environment with:\n",
    "\n",
    "  ```sh\n",
    "  pip install ehrapy[dask] scalene\n",
    "  ```\n",
    "\n",
    "  >>> 2. Profile runs\n",
    "  >>> Scalene currently requires code to be run as Python script for a full profile. For this, copy the above code snippets into two Python files \"profile_memory.py\" and \"profile_out_of_core.py\", respectively.\n",
    "  >>> Then, from your commmand line within this environment run\n",
    "\n",
    "  ```sh\n",
    "  scalene profile_memory.py --outfile profile_memory.html\n",
    "  ```\n",
    "\n",
    "  >>> for the in-memory computation and \n",
    "\n",
    "  ```sh\n",
    "  scalene profile_out_of_core.py --outfile profile_out_of_core.html\n",
    "  ```\n",
    "  \n",
    "  >>> for the out-of-core computation.\n",
    "  </summary>\n",
    "  <!-- Element that had a CSS class selector -->\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "The resulting `Scalene` profiles can depend on the machine and environment the profiling is run. Here, we show results obtained on an Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz laptop.\n",
    "\n",
    "#### In Memory Profile\n",
    "\n",
    "![](images/memory_scalene_profile_50000x1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-core Profile\n",
    "![](images/ooc_scalene_profile_50000x1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple features in `scalene`'s output, we show and focus on the key aspects here.\n",
    "\n",
    "#### Time\n",
    "The required time for the profiled code to execute is displayed at the topmost \"% of time = 100.0%\". We can see that for the in-memory computation, the required wall time was 1 min 41 seconds, whereas for the out-of-core computation, the required wall time was 1 min 39 seconds.\n",
    "\n",
    "Generally, the out-of-core computation can yield performance improvements by optimizing the scheduling of computations by using \"lazy execution\", for which you can find more information [here](https://docs.dask.org/en/stable/scheduling.html). `Dask` also allows [distributed computations](https://distributed.dask.org/en/stable/), providing even further speed improvements.\n",
    "\n",
    "Beware that especially for small dataset or small chunk sizes, the overhead of such out-of-core computations can also exceed the gains obtained.\n",
    "\n",
    "Here, we can see that the order of magnitude for execution time for both workflows is similar.\n",
    "\n",
    "#### Maximum Memory Consumption\n",
    "The maximum required memory for the profiled code is displayed at the top right, on top of the \"Memory timeline\" plot.\n",
    "We can see that the maximum memory occupation for the in-memory computation was 2 GB, whereas for the out-of-core computation, the maximum memory occupation was 193 MB.\n",
    "\n",
    "This can be achieved by leveraging the key idea behind `dask`, its [block-wise](https://docs.dask.org/en/latest/array-chunks.html) (\"chunked\") computations. This means that `dask` accesses the data in chunks, not requiring the entire dataset to be loaded to memory at once.\n",
    "The size of the blocks is a tradeoff of having not too many small blocks (causing too much overhead) and not too big blocks (causing increased memory consumption and potentially less distributed computations). See [here](https://docs.dask.org/en/latest/array-chunks.html) for a more detailed discussion on the block size.\n",
    "\n",
    "### Conclusion\n",
    "Here, we show with a synthetic dataset how out-of-core computations can reduce the memory requirements for a suite of analysis steps.\n",
    "\n",
    "The dataset we have generated is small enough to not require out-of-core computations, so we can compare the in-memory and the out-of-core computation profiles.\n",
    "\n",
    "In general, in-memory computations are to be preferred whenever they are feasible, being easier to perform and omit potential pitfals such as using too small chunk sizes.\n",
    "\n",
    "We have computed a PCA and can observe that the four underlying groups in our synthetic data form well separated clusters in the two-dimensional PCA projection, and are well clustered by Leiden clustering. \n",
    "\n",
    "\n",
    "![](images/pcaprofiling_out_of_core_pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further support for `dask` is a work in progress. However, many operations past this point can work with the dimensionality reduction directly in memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehrapy_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
