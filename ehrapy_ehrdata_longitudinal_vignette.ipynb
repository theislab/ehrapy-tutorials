{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d645defb",
   "metadata": {},
   "source": [
    "# Longitudinal EHR Data Analysis with ehrapy and ehrdata\n",
    "\n",
    "This notebook demonstrates how to analyze longitudinal electronic health records (EHR) data using `ehrapy` and `ehrdata`. We will use the Physionet2012 Challenge dataset to showcase:\n",
    "\n",
    "- Data loading and description\n",
    "- Cohort tracking\n",
    "- Longitudinal feature imputation\n",
    "- Time series visualization\n",
    "- Sankey diagrams (static and time-based)\n",
    "- Machine learning for representation learning\n",
    "- Clustering with Leiden algorithm\n",
    "- UMAP visualization\n",
    "- Feature ranking based on clusters\n",
    "\n",
    "The Physionet2012 Challenge dataset contains ICU stay data with 37 numeric features measured over 48 hours for approximately 4,000 patients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ca947",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ehrdata as ed\n",
    "import ehrapy as ep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b92f76",
   "metadata": {},
   "source": [
    "## Data Loading and Description\n",
    "\n",
    "We load the Physionet2012 Challenge dataset using `ehrdata`. This dataset contains temporal data in a 3D format (patients × features × timepoints).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Physionet2012 dataset\n",
    "edata = ed.dt.physionet2012(layer=\"tem_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aaa015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", edata.shape)\n",
    "print(\"Number of patients:\", edata.n_obs)\n",
    "print(\"Number of features:\", edata.n_vars)\n",
    "print(\"Number of timepoints:\", edata.tem.shape[0] if hasattr(edata, 'tem') else \"N/A\")\n",
    "print(\"\\nFeature names:\")\n",
    "print(edata.var_names[:10])  # Show first 10 features\n",
    "print(\"\\nObservation metadata columns:\")\n",
    "print(list(edata.obs.columns)[:10])  # Show first 10 obs columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc022b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset object\n",
    "edata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8acf409",
   "metadata": {},
   "source": [
    "## Cohort Tracking\n",
    "\n",
    "We use `CohortTracker` to monitor changes in patient demographics and characteristics throughout the analysis pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0982e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns for tracking\n",
    "categorical_cols = []\n",
    "for col in edata.obs.columns:\n",
    "    if edata.obs[col].dtype == 'object' or edata.obs[col].dtype.name == 'category':\n",
    "        unique_vals = edata.obs[col].nunique()\n",
    "        if unique_vals < 20:  # Consider columns with < 20 unique values as categorical\n",
    "            categorical_cols.append(col)\n",
    "\n",
    "print(\"Categorical columns for tracking:\", categorical_cols[:5])  # Show first 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146dacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CohortTracker\n",
    "# We'll track a subset of columns that are available in the dataset\n",
    "tracking_cols = [col for col in ['Gender', 'Age', 'In-hospital_death'] if col in edata.obs.columns]\n",
    "if len(tracking_cols) == 0:\n",
    "    # If those columns don't exist, use available columns\n",
    "    tracking_cols = list(edata.obs.columns)[:5]\n",
    "\n",
    "ct = ep.tl.CohortTracker(edata, columns=tracking_cols, categorical=categorical_cols[:3] if categorical_cols else None)\n",
    "\n",
    "# Track initial state\n",
    "ct(edata, label=\"Initial cohort\", operations_done=\"Data loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc37e8e",
   "metadata": {},
   "source": [
    "## Longitudinal Feature Imputation\n",
    "\n",
    "For longitudinal data, we can use imputation methods that work across the time axis. We'll use `simple_impute` with a layer parameter to impute missing values in the temporal data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before imputation\n",
    "if \"tem_data\" in edata.layers:\n",
    "    missing_before = np.isnan(edata.layers[\"tem_data\"]).sum()\n",
    "    total_values = edata.layers[\"tem_data\"].size\n",
    "    print(f\"Missing values before imputation: {missing_before} ({100*missing_before/total_values:.2f}%)\")\n",
    "    \n",
    "    # Perform longitudinal imputation using mean strategy\n",
    "    ep.pp.simple_impute(edata, layer=\"tem_data\", strategy=\"mean\")\n",
    "    \n",
    "    # Check missing values after imputation\n",
    "    missing_after = np.isnan(edata.layers[\"tem_data\"]).sum()\n",
    "    print(f\"Missing values after imputation: {missing_after} ({100*missing_after/total_values:.2f}%)\")\n",
    "    \n",
    "    # Track after imputation\n",
    "    ct(edata, label=\"After imputation\", operations_done=\"Longitudinal imputation (mean)\")\n",
    "else:\n",
    "    print(\"tem_data layer not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fd23d",
   "metadata": {},
   "source": [
    "## Time Series Visualization\n",
    "\n",
    "We can visualize time series data for individual patients and features using `ep.pl.timeseries`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for a few patients and features\n",
    "if \"tem_data\" in edata.layers and hasattr(edata, 'tem'):\n",
    "    # Select a few patients and features to visualize\n",
    "    n_patients = min(3, edata.n_obs)\n",
    "    n_features = min(3, edata.n_vars)\n",
    "    \n",
    "    plot = ep.pl.timeseries(\n",
    "        edata,\n",
    "        obs_names=slice(0, n_patients),\n",
    "        var_names=slice(0, n_features),\n",
    "        layer=\"tem_data\",\n",
    "        overlay=False,\n",
    "        title=\"Time Series for Selected Patients and Features\"\n",
    "    )\n",
    "    hv.render(plot)\n",
    "else:\n",
    "    print(\"Temporal data layer not available for plotting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b52b8",
   "metadata": {},
   "source": [
    "## Sankey Diagrams\n",
    "\n",
    "Sankey diagrams are useful for visualizing flows and transitions. We'll create both a standard Sankey diagram and a time-based Sankey diagram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Sankey diagram for categorical variables in obs\n",
    "# Find categorical columns that can be used for Sankey\n",
    "sankey_cols = []\n",
    "for col in edata.obs.columns:\n",
    "    if col in categorical_cols or edata.obs[col].nunique() < 10:\n",
    "        sankey_cols.append(col)\n",
    "        if len(sankey_cols) >= 3:\n",
    "            break\n",
    "\n",
    "if len(sankey_cols) >= 2:\n",
    "    sankey_plot = ep.pl.sankey_diagram(edata, columns=sankey_cols[:3], title=\"Patient Flow Across Categories\")\n",
    "    hv.render(sankey_plot)\n",
    "else:\n",
    "    print(\"Not enough categorical columns for Sankey diagram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based Sankey diagram for a categorical variable\n",
    "# First, we need to identify or create a categorical variable in the temporal data\n",
    "# For demonstration, we'll use a feature that can be discretized\n",
    "\n",
    "if \"tem_data\" in edata.layers and edata.n_vars > 0:\n",
    "    # Use the first feature and discretize it into states\n",
    "    # This is a simplified example - in practice, you'd use an actual categorical feature\n",
    "    tem_data = edata.layers[\"tem_data\"]\n",
    "    \n",
    "    # Discretize the first feature into 3 states (low, medium, high)\n",
    "    feature_idx = 0\n",
    "    feature_data = tem_data[:, feature_idx, :]\n",
    "    \n",
    "    # Create states based on percentiles\n",
    "    for t in range(feature_data.shape[1]):\n",
    "        values = feature_data[:, t]\n",
    "        valid_values = values[~np.isnan(values)]\n",
    "        if len(valid_values) > 0:\n",
    "            p33 = np.nanpercentile(valid_values, 33)\n",
    "            p66 = np.nanpercentile(valid_values, 66)\n",
    "            \n",
    "            # Create discrete states\n",
    "            states = np.zeros_like(values, dtype=int)\n",
    "            states[values < p33] = 0\n",
    "            states[(values >= p33) & (values < p66)] = 1\n",
    "            states[values >= p66] = 2\n",
    "            \n",
    "            # Store in a temporary layer for visualization\n",
    "            if t == 0:\n",
    "                discrete_layer = np.zeros((edata.n_obs, 1, feature_data.shape[1]), dtype=int)\n",
    "            discrete_layer[:, 0, t] = states\n",
    "    \n",
    "    # Create a temporary edata with discrete states\n",
    "    edata_temp = edata.copy()\n",
    "    edata_temp.layers[\"discrete_states\"] = discrete_layer\n",
    "    edata_temp.var_names = [\"state_feature\"]\n",
    "    \n",
    "    # Create state labels\n",
    "    state_labels = {0: \"Low\", 1: \"Medium\", 2: \"High\"}\n",
    "    \n",
    "    # Plot time-based Sankey\n",
    "    sankey_time_plot = ep.pl.sankey_diagram_time(\n",
    "        edata_temp,\n",
    "        var_name=\"state_feature\",\n",
    "        layer=\"discrete_states\",\n",
    "        state_labels=state_labels,\n",
    "        title=\"State Transitions Over Time\"\n",
    "    )\n",
    "    hv.render(sankey_time_plot)\n",
    "else:\n",
    "    print(\"Temporal data not available for time-based Sankey diagram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36585974",
   "metadata": {},
   "source": [
    "## Machine Learning for Representation Learning\n",
    "\n",
    "We'll use a simple approach to learn patient representations from the temporal data. For this, we'll extract features at a specific timepoint and use them for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patient representations at the final timepoint (or a specific timepoint)\n",
    "# This creates a 2D representation from the 3D temporal data\n",
    "if \"tem_data\" in edata.layers:\n",
    "    # Use the last timepoint for representation\n",
    "    final_timepoint = edata.layers[\"tem_data\"][:, :, -1]  # Shape: (n_obs, n_vars)\n",
    "    \n",
    "    # Store in obsm for downstream analysis\n",
    "    edata.obsm[\"X_final_timepoint\"] = final_timepoint\n",
    "    \n",
    "    # Also create a mean representation across all timepoints\n",
    "    mean_representation = np.nanmean(edata.layers[\"tem_data\"], axis=2)\n",
    "    edata.obsm[\"X_mean_timepoint\"] = mean_representation\n",
    "    \n",
    "    print(f\"Created representations:\")\n",
    "    print(f\"  - Final timepoint: {edata.obsm['X_final_timepoint'].shape}\")\n",
    "    print(f\"  - Mean across timepoints: {edata.obsm['X_mean_timepoint'].shape}\")\n",
    "else:\n",
    "    print(\"Temporal data layer not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7b621",
   "metadata": {},
   "source": [
    "## Leiden Clustering\n",
    "\n",
    "We'll perform Leiden clustering on the patient representations. First, we need to compute a neighborhood graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a630d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighbors using the mean representation\n",
    "if \"X_mean_timepoint\" in edata.obsm:\n",
    "    ep.pp.neighbors(edata, use_rep=\"X_mean_timepoint\", n_neighbors=15, n_pcs=None)\n",
    "    \n",
    "    # Perform Leiden clustering\n",
    "    ep.tl.leiden(edata, resolution=0.5, key_added=\"leiden\")\n",
    "    \n",
    "    print(f\"Leiden clusters computed. Number of clusters: {edata.obs['leiden'].nunique()}\")\n",
    "    print(f\"Cluster distribution:\\n{edata.obs['leiden'].value_counts().head()}\")\n",
    "    \n",
    "    # Track after clustering\n",
    "    ct(edata, label=\"After clustering\", operations_done=\"Leiden clustering\")\n",
    "else:\n",
    "    print(\"Representations not available for clustering\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449fcea",
   "metadata": {},
   "source": [
    "## UMAP Visualization\n",
    "\n",
    "We'll compute and visualize UMAP embeddings to explore the patient representations in 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9cd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP embedding\n",
    "if \"neighbors\" in edata.uns:\n",
    "    ep.tl.umap(edata)\n",
    "    \n",
    "    # Visualize UMAP with Leiden clusters\n",
    "    ep.pl.umap(edata, color=\"leiden\", title=\"UMAP colored by Leiden clusters\", show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also visualize with other metadata if available\n",
    "    if \"In-hospital_death\" in edata.obs.columns:\n",
    "        ep.pl.umap(edata, color=[\"leiden\", \"In-hospital_death\"], title=[\"Leiden clusters\", \"In-hospital death\"], show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Neighbors not computed. Please run ep.pp.neighbors first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06972d",
   "metadata": {},
   "source": [
    "## Feature Ranking Based on Leiden Clusters\n",
    "\n",
    "We'll identify features that are differentially expressed across Leiden clusters using `rank_features_groups`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank features based on Leiden clusters\n",
    "# We'll use the mean representation stored in X for ranking\n",
    "if \"leiden\" in edata.obs.columns:\n",
    "    # Set X to the mean representation for ranking\n",
    "    edata.X = edata.obsm[\"X_mean_timepoint\"].copy()\n",
    "    \n",
    "    # Rank features\n",
    "    ep.tl.rank_features_groups(edata, groupby=\"leiden\", n_features=10)\n",
    "    \n",
    "    # Visualize top ranked features\n",
    "    ep.pl.rank_features_groups(edata, n_features=5, key=\"rank_features_groups\", show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get ranked features as DataFrame\n",
    "    for cluster in sorted(edata.obs[\"leiden\"].unique())[:3]:  # Show top 3 clusters\n",
    "        df = ep.get.rank_features_groups_df(edata, group=cluster, key=\"rank_features_groups\")\n",
    "        print(f\"\\nTop features for cluster {cluster}:\")\n",
    "        print(df.head())\n",
    "else:\n",
    "    print(\"Leiden clusters not found. Please run clustering first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d11f1",
   "metadata": {},
   "source": [
    "## Cohort Tracking Summary\n",
    "\n",
    "Let's visualize the cohort changes tracked throughout the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d225b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cohort tracking barplot\n",
    "if len(ct._tracked_tables) > 1:\n",
    "    try:\n",
    "        ct.plot_cohort_barplot()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot cohort barplot: {e}\")\n",
    "        print(\"This is expected if the tracked columns don't have the right format\")\n",
    "else:\n",
    "    print(\"Not enough tracking steps to plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3477d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading**: Loading and exploring the Physionet2012 longitudinal EHR dataset\n",
    "2. **Cohort Tracking**: Monitoring patient cohort changes throughout the analysis\n",
    "3. **Longitudinal Imputation**: Handling missing values in temporal data\n",
    "4. **Time Series Visualization**: Plotting patient trajectories over time\n",
    "5. **Sankey Diagrams**: Visualizing flows between categories and state transitions over time\n",
    "6. **Representation Learning**: Extracting patient representations from temporal data\n",
    "7. **Leiden Clustering**: Identifying patient subgroups\n",
    "8. **UMAP Visualization**: Exploring patient relationships in 2D space\n",
    "9. **Feature Ranking**: Identifying features that distinguish between clusters\n",
    "\n",
    "These tools enable comprehensive analysis of longitudinal EHR data, from preprocessing to visualization and interpretation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
